{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import json\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "#from pyspark.sql import sparkContext\n",
        "\n",
        "def parse_csv(line):\n",
        "    record_type_pos = 2\n",
        "    record = line.split(\",\")\n",
        "    try:\n",
        "        if record[record_type_pos] == 'T':\n",
        "              return (datetime.strptime(record[0], '%Y-%m-%d').date(),\n",
        "                      record[2],\n",
        "                      record[3],\n",
        "                      record[6],\n",
        "                      datetime.strptime(record[4], '%Y-%m-%d %H:%M:%S.%f'),\n",
        "                      int(record[5]),\n",
        "                      datetime.strptime(record[1], '%Y-%m-%d %H:%M:%S.%f'),\n",
        "                      float(record[7]),\n",
        "                      None,\n",
        "                      None,\n",
        "                      None,\n",
        "                      None,\n",
        "                      record[2])\n",
        "        elif record[record_type_pos] == 'Q':\n",
        "              return (datetime.strptime(record[0], '%Y-%m-%d').date(),\n",
        "                      record[2],\n",
        "                      record[3],\n",
        "                      record[6],\n",
        "                      datetime.strptime(record[4], '%Y-%m-%d %H:%M:%S.%f'),\n",
        "                      int(record[5]),\n",
        "                      datetime.strptime(record[1], '%Y-%m-%d %H:%M:%S.%f'),\n",
        "                      None,\n",
        "                      float(record[7]),\n",
        "                      int(record[8]),\n",
        "                      float(record[9]),\n",
        "                      int(record[10]),\n",
        "                      record[2])\n",
        "    except:\n",
        "        return (None, None, None, None, None, None, None, None, None, None, None, None, 'B')\n",
        "\n",
        "\n",
        "def parse_json(line):\n",
        "    line = json.loads(line)\n",
        "    record_type = line['event_type']\n",
        "    try:\n",
        "        if record_type == 'T':\n",
        "              return (datetime.strptime(line['trade_dt'], '%Y-%m-%d').date(),\n",
        "                      record_type,\n",
        "                      line['symbol'],\n",
        "                      line['exchange'],\n",
        "                      datetime.strptime(line['event_tm'], '%Y-%m-%d %H:%M:%S.%f'),\n",
        "                      line['event_seq_nb'],\n",
        "                      datetime.strptime(line['file_tm'], '%Y-%m-%d %H:%M:%S.%f'),\n",
        "                      line['price'],\n",
        "                      None,\n",
        "                      None,\n",
        "                      None,\n",
        "                      None,\n",
        "                      record_type)\n",
        "        elif record_type == 'Q':\n",
        "              return (datetime.strptime(line['trade_dt'], '%Y-%m-%d').date(),\n",
        "                      record_type,\n",
        "                      line['symbol'],\n",
        "                      line['exchange'],\n",
        "                      datetime.strptime(line['event_tm'], '%Y-%m-%d %H:%M:%S.%f'),\n",
        "                      line['event_seq_nb'],\n",
        "                      datetime.strptime(line['file_tm'], '%Y-%m-%d %H:%M:%S.%f'),\n",
        "                      None,\n",
        "                      line['bid_pr'],\n",
        "                      line['bid_size'],\n",
        "                      line['ask_pr'],\n",
        "                      line['ask_size'],\n",
        "                      record_type)\n",
        "    except:\n",
        "        return (None, None, None, None, None, None, None, None, None, None, None, None, 'B')\n",
        "\n",
        "common_event_schema = StructType([StructField('trade_dt', DateType(), True),\n",
        "                           StructField('rec_type', StringType(), True),\n",
        "                           StructField('symbol', StringType(), True),\n",
        "                           StructField('exchange', StringType(), True),\n",
        "                           StructField('event_tm', TimestampType(), True),\n",
        "                           StructField('event_seq_nb', IntegerType(), True),\n",
        "                           StructField('arrival_tm', TimestampType(), True),\n",
        "                           StructField('trade_pr', FloatType(), True),\n",
        "                           StructField('bid_pr', FloatType(), True),\n",
        "                           StructField('bid_size', IntegerType(), True),\n",
        "                           StructField('ask_pr', FloatType(), True),\n",
        "                           StructField('ask_size', IntegerType(), True),\n",
        "                           StructField('partition', StringType(), True)])\n",
        "\n",
        "dates = ['2020-08-05','2020-08-06']\n",
        "csvlist = []\n",
        "jsonlist = []\n",
        "#spark = SparkSession.builder.master(‘local’).appName(‘app’).getOrCreate()\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "spark.conf.set(\"fs.azure.account.key.asastorewin.blob.core.windows.net\",\"k+/sXvVVFPVSeOuCPMW8HUq9uqvczt4Ya0fQdPGxRepxIVnQ5iJtIXYxw14Y9Y5M5YmvyIs+Cuy8sXZwIn7eXQ==\")\n",
        "for dt in dates:\n",
        "    rawcsv = spark.sparkContext.textFile(f\"wasbs://newestcontainer@asastorewin.blob.core.windows.net/data/csv/%s/NYSE/*.txt\" %dt)\n",
        "    rawjson = spark.sparkContext.textFile(f\"wasbs://newestcontainer@asastorewin.blob.core.windows.net/data/json/%s/NASDAQ/*.txt\" %dt)\n",
        "    parsedcsv = rawcsv.map(lambda line: parse_csv(line))\n",
        "    parsedjson = rawjson.map(lambda line: parse_json(line))\n",
        "    datacsv = spark.createDataFrame(parsedcsv, common_event_schema)\n",
        "    datajson = spark.createDataFrame(parsedjson, common_event_schema)\n",
        "    csvlist.append(datacsv)\n",
        "    jsonlist.append(datajson)\n",
        "\n",
        "csv_data = csvlist[0].union(csvlist[1])\n",
        "json_data= jsonlist[0].union(jsonlist[1])\n",
        "all_data = csv_data.union(json_data)\n",
        "\n",
        "print('csv data: ')\n",
        "csv_data.show()\n",
        "\n",
        "print('\\n\\njson data: ')\n",
        "json_data.show()\n",
        "\n",
        "#json_data.write.partitionBy(\"partition\").mode(\"overwrite\").parquet(\"wasbs://newestcontainer@asastorewin.blob.core.windows.net/output_dir\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool01",
              "session_id": 2,
              "statement_id": 43,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-05-24T00:02:57.3282243Z",
              "session_start_time": null,
              "execution_start_time": "2022-05-24T00:02:57.4315119Z",
              "execution_finish_time": "2022-05-24T00:02:57.5736635Z"
            },
            "text/plain": "StatementMeta(SparkPool01, 2, 43, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 41,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#subset of data from printing (rand out of compute after making a change and can no longer output)\n",
        "#+----------+--------------------+--------+------+--------------------+------------+--------+------+--------+------+--------+---------+\n",
        "#|  trade_dt|          arrival_tm|rec_type|symbol|            event_tm|event_seq_nb|trade_pr|bid_pr|bid_size|ask_pr|ask_size|partition|\n",
        "#+----------+--------------------+--------+------+--------------------+------------+--------+------+--------+------+--------+---------+\n",
        "#|2020-08-06|2020-08-06 09:30:...|       Q|  SYMA|2020-08-06 09:39:...|           1|    NYSE|    78|     100|    78|     100|        Q|\n",
        "#|2020-08-06|2020-08-06 09:30:...|       Q|  SYMA|2020-08-06 09:47:...|           2|    NYSE|    77|     100|    77|     100|        Q|\n",
        "#|2020-08-06|2020-08-06 09:30:...|       Q|  SYMA|2020-08-06 09:56:...|           3|    NYSE|    75|     100|    75|     100|        Q|\n",
        "#+----------+--------------------+--------+------+--------------------+------------+--------+------+--------+------+--------+---------+"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
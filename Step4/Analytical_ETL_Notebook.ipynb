{"cells":[{"cell_type":"code","source":["from datetime import datetime\nimport json\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"10cb0cbc-5e91-4f29-a22d-a25cc28e8216"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Select The Necessary Columns For Trade Records\ntrade = trade_common.select(\"trade_dt\", \"rec_type\", \"symbol\", \"exchange\", \"event_tm\", \"event_seq_nb\", \"arrival_tm\", \"trade_pr\")\n\n# Apply Data Correction\ndef applyLatest(df):\n    #trades\n    if df.first()[\"rec_type\"] == \"T\":\n        df_grouped = df.groupBy(\"trade_dt\", \"rec_type\", \"symbol\", \"arrival_tm\", \"event_seq_nb\").agg(max(\"event_tm\").alias(\"latest_trade\"))\n        df_joined = df_grouped.join(df.select(\"event_tm\", \"exchange\", \"trade_pr\"), df.event_tm == df_grouped.latest_trade, \"inner\")\n        df_final = df_joined.select(\"trade_dt\", \"rec_type\", col(\"symbol\").alias(\"stock_symbol\"), col(\"exchange\").alias(\"stock_exchange\"), \"latest_trade\", \"event_seq_nb\", \"arrival_tm\", \"trade_pr\").orderBy(\"trade_dt\", \"symbol\", \"event_seq_nb\")\n        return df_final\n    #quotes\n    elif df.first()[\"rec_type\"] == \"Q\":\n        df_grouped = df.groupBy(\"trade_dt\", \"rec_type\", \"symbol\", \"arrival_tm\", \"event_seq_nb\").agg(max(\"event_tm\").alias(\"latest_quote\"))\n        df_joined = df_grouped.join(df.select(\"event_tm\", \"exchange\", \"bid_pr\", \"bid_size\", \"ask_pr\", \"ask_size\"), df.event_tm == df_grouped.latest_quote, \"inner\")\n        df_final = df_joined.select(\"trade_dt\", \"rec_type\", col(\"symbol\").alias(\"stock_symbol\"), col(\"exchange\").alias(\"stock_exchange\"), \"latest_quote\", \"event_seq_nb\", \"arrival_tm\", \"bid_pr\", \"bid_size\", \"ask_pr\", \"ask_size\").orderBy(\"trade_dt\", \"symbol\", \"event_seq_nb\")\n        return df_final\n\ntrade_corrected = applyLatest(trade)\ntrades_080520 = trade_corrected.where(trade_corrected.trade_dt == \"2020-08-05\")\ntrades_080620 = trade_corrected.where(trade_corrected.trade_dt == \"2020-08-06\")\n\n#  Write The Trade Dataset Back To Parquet On Azure Blob Storage\ntrades_080520.write.mode('overwrite').parquet(\"/trade/trade_dt={}\".format('2020-08-05'))\ntrades_080620.write.mode('overwrite').parquet(\"/trade/trade_dt={}\".format('2020-08-06'))\n\n# ****************** REPEAT FOR QUOTES ******************\nquote_common = spark.read.format('parquet').load(\"/output_dir/partition=Q\")\n\n# Select The Necessary Columns For Quote Records\nquote = quote_common.select(\"trade_dt\", \"rec_type\", \"symbol\", \"exchange\", \"event_tm\", \"event_seq_nb\", \"arrival_tm\", \"bid_pr\", \"bid_size\", \"ask_pr\", \"ask_size\")\n\n# Apply Data Correction\nquote_corrected = applyLatest(quote)\n\n# Separate dataframes by trade date\nquotes_080520 = quote_corrected.where(quote_corrected.trade_dt == \"2020-08-05\")\nquotes_080620 = quote_corrected.where(quote_corrected.trade_dt == \"2020-08-06\")\n\n# Write The Quote Dataset Back To Parquet On Azure Blob Storage\nquotes_080520.write.mode('overwrite').parquet(\"/quote/trade_dt={}\".format('2020-08-05'))\nquotes_080620.write.mode('overwrite').parquet(\"/quote/trade_dt={}\".format('2020-08-06'))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"56328945-205b-46f9-a6d3-8d241720b686"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# 4.1 Read Parquet Files From Azure Blob Storage Partition\n# df = spark.read.parquet(\"cloud-storage-path/trade/date={}\".format(\"2020-07-29\"))\ndf =spark.read.parquet(\"/trade/trade_dt={}\".format(\"2020-08-05\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cf36bbdb-88b4-4f98-92fa-c1cfef5fba0c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+------------+--------------------+------------+---------+\n|stock_symbol|        latest_trade|event_seq_nb| trade_pr|\n+------------+--------------------+------------+---------+\n|        SYMA|2020-08-05 10:38:...|          10|  77.7757|\n|        SYMA|2020-08-05 11:58:...|          20|75.715225|\n|        SYMA|2020-08-05 13:09:...|          30| 75.87926|\n+------------+--------------------+------------+---------+\nonly showing top 3 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------+--------------------+------------+---------+\nstock_symbol|        latest_trade|event_seq_nb| trade_pr|\n+------------+--------------------+------------+---------+\n        SYMA|2020-08-05 10:38:...|          10|  77.7757|\n        SYMA|2020-08-05 11:58:...|          20|75.715225|\n        SYMA|2020-08-05 13:09:...|          30| 75.87926|\n+------------+--------------------+------------+---------+\nonly showing top 3 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# 4.2 Create Trade Staging Table\n# 4.2.1 Use Spark To Read The Trade Table With Date Partition “2020-07-29”\n# df = spark.sql(\"select symbol, event_tm, event_seq_nb, trade_pr from trades where trade_dt = '2020-07-29'\")\ndf = df.select(\"stock_symbol\",\"latest_trade\", \"event_seq_nb\", \"trade_pr\").where(df.trade_dt == \"2020-08-05\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"faa1263d-f18c-4813-9eed-8dc84730f1b3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# 4.2.2 Create A Spark Temporary View\ndf.createOrReplaceTempView(\"tmp_trade_moving_avg\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a04eb439-3b88-49a2-84c0-3ce3e31999ae"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["\"\"\"\n4.2.3 Calculate The 30-min Moving Average Using The Spark Temp View \nPartition by symbol and order by time. The window should contain all records within 30-min of the corresponding row.\nmov_avg_df = spark.sql('select symbol, exchange, event_tm, event_seq_nb, trade_pr,# [logic to derive last 30 min moving average price] as mov_avg_pr\\from tmp_trade_moving_avg')\n\"\"\"\nmov_avg_df = spark.sql('select stock_symbol, latest_trade, event_seq_nb, trade_pr,() as mov_avg_pr from tmp_trade_moving_avg')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c3938d60-12ac-48e8-82ab-f861daa30874"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Analytical_ETL_Notebook","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":718919565017151}},"nbformat":4,"nbformat_minor":0}

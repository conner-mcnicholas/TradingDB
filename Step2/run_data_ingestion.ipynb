{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "53eca822-58ec-4062-9df0-3ee8b7733209",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (find_spark_home.py, line 68)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/opt/spark/python/pyspark/find_spark_home.py\"\u001b[0;36m, line \u001b[0;32m68\u001b[0m\n\u001b[0;31m    print(\"Could not find valid SPARK_HOME while searching {0}\".format(paths), file=sys.stderr)\u001b[0m\n\u001b[0m                                                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "#from pyspark.sql import sparkContext\n",
    "\n",
    "def parse_csv(line):\n",
    "    record_type_pos = 2\n",
    "    record = line.split(\",\")\n",
    "    try:\n",
    "        if record[record_type_pos] == 'T':\n",
    "              return (datetime.strptime(record[0], '%Y-%m-%d').date(),\n",
    "                      record[2],\n",
    "                      record[3],\n",
    "                      record[6],\n",
    "                      datetime.strptime(record[4], '%Y-%m-%d %H:%M:%S.%f'),\n",
    "                      int(record[5]),\n",
    "                      datetime.strptime(record[1], '%Y-%m-%d %H:%M:%S.%f'),\n",
    "                      float(record[7]),\n",
    "                      None,\n",
    "                      None,\n",
    "                      None,\n",
    "                      None,\n",
    "                      record[2])\n",
    "        elif record[record_type_pos] == 'Q':\n",
    "              return (datetime.strptime(record[0], '%Y-%m-%d').date(),\n",
    "                      record[2],\n",
    "                      record[3],\n",
    "                      record[6],\n",
    "                      datetime.strptime(record[4], '%Y-%m-%d %H:%M:%S.%f'),\n",
    "                      int(record[5]),\n",
    "                      datetime.strptime(record[1], '%Y-%m-%d %H:%M:%S.%f'),\n",
    "                      None,\n",
    "                      float(record[7]),\n",
    "                      int(record[8]),\n",
    "                      float(record[9]),\n",
    "                      int(record[10]),\n",
    "                      record[2])\n",
    "    except:\n",
    "        return (None, None, None, None, None, None, None, None, None, None, None, None, 'B')\n",
    "\n",
    "\n",
    "def parse_json(line):\n",
    "    line = json.loads(line)\n",
    "    record_type = line['event_type']\n",
    "    try:\n",
    "        if record_type == 'T':\n",
    "              return (datetime.strptime(line['trade_dt'], '%Y-%m-%d').date(),\n",
    "                      record_type,\n",
    "                      line['symbol'],\n",
    "                      line['exchange'],\n",
    "                      datetime.strptime(line['event_tm'], '%Y-%m-%d %H:%M:%S.%f'),\n",
    "                      line['event_seq_nb'],\n",
    "                      datetime.strptime(line['file_tm'], '%Y-%m-%d %H:%M:%S.%f'),\n",
    "                      line['price'],\n",
    "                      None,\n",
    "                      None,\n",
    "                      None,\n",
    "                      None,\n",
    "                      record_type)\n",
    "        elif record_type == 'Q':\n",
    "              return (datetime.strptime(line['trade_dt'], '%Y-%m-%d').date(),\n",
    "                      record_type,\n",
    "                      line['symbol'],\n",
    "                      line['exchange'],\n",
    "                      datetime.strptime(line['event_tm'], '%Y-%m-%d %H:%M:%S.%f'),\n",
    "                      line['event_seq_nb'],\n",
    "                      datetime.strptime(line['file_tm'], '%Y-%m-%d %H:%M:%S.%f'),\n",
    "                      None,\n",
    "                      line['bid_pr'],\n",
    "                      line['bid_size'],\n",
    "                      line['ask_pr'],\n",
    "                      line['ask_size'],\n",
    "                      record_type)\n",
    "    except:\n",
    "        return (None, None, None, None, None, None, None, None, None, None, None, None, 'B')\n",
    "\n",
    "common_event_schema = StructType([StructField('trade_dt', DateType(), True),\n",
    "                           StructField('rec_type', StringType(), True),\n",
    "                           StructField('symbol', StringType(), True),\n",
    "                           StructField('exchange', StringType(), True),\n",
    "                           StructField('event_tm', TimestampType(), True),\n",
    "                           StructField('event_seq_nb', IntegerType(), True),\n",
    "                           StructField('arrival_tm', TimestampType(), True),\n",
    "                           StructField('trade_pr', FloatType(), True),\n",
    "                           StructField('bid_pr', FloatType(), True),\n",
    "                           StructField('bid_size', IntegerType(), True),\n",
    "                           StructField('ask_pr', FloatType(), True),\n",
    "                           StructField('ask_size', IntegerType(), True),\n",
    "                           StructField('partition', StringType(), True)])\n",
    "\n",
    "dates = ['2020-08-05','2020-08-06']\n",
    "csvlist = []\n",
    "jsonlist = []\n",
    "#spark = SparkSession.builder.master(‘local’).appName(‘app’).getOrCreate()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.conf.set(\"fs.azure.account.key.pipelinestorageacctaus.blob.core.windows.net\",\"accessKey\")\n",
    "for dt in dates:\n",
    "    rawcsv = spark.sparkContext.textFile( \"wasbs://pipelineauscontainer@pipelinestorageacctaus.blob.core.windows.net/data/csv/%s/NYSE/*.txt\" %dt)\n",
    "    rawjson = spark.sparkContext.textFile( \"wasbs://pipelineauscontainer@pipelinestorageacctaus.blob.core.windows.net/data/json/%s/NASDAQ/*.txt\" %dt)\n",
    "    parsedcsv = rawcsv.map(lambda line: parse_csv(line))\n",
    "    parsedjson = rawjson.map(lambda line: parse_json(line))\n",
    "    datacsv = spark.createDataFrame(parsedcsv, common_event_schema)\n",
    "    datajson = spark.createDataFrame(parsedjson, common_event_schema)\n",
    "    csvlist.append(datacsv)\n",
    "    jsonlist.append(datajson)\n",
    "\n",
    "csv_data = csvlist[0].union(csvlist[1])\n",
    "json_data= jsonlist[0].union(jsonlist[1])\n",
    "all_data = csv_data.union(json_data)\n",
    "\n",
    "print('all parsed and combined data: ')\n",
    "all_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "48f051d1-ac21-43b7-9e6c-cad5da7e328f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.write.format(\"parquet\").saveAsTable(\"all_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "759b87db-1ca2-4f44-bdcd-d31f83c8142e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_data.write.partitionBy(\"partition\").mode(\"overwrite\").parquet(\"wasbs://newestcontainer@asastorewin.blob.core.windows.net/output_dir\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "2022-05-24 - Azure Blob Storage Import Example Notebook",
   "notebookOrigID": 3824210079242,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
